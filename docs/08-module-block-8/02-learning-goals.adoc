=== {learning-goals}

// tag::DE[]

[[LZ-8-1]]
==== LZ 8-1: Ökologische Auswirkungen von KI-Systemen kennen

Die Teilnehmenden verstehen die ökologischen Auswirkungen von KI-Systemen, insbesondere die von generativer künstlicher Intelligenz mit großen Sprachmodellen, über ihren gesamten Lebenszyklus hinweg. Sie können die Phasen Datensammlung, Training, Deployment, Inferenz und Wartung unterscheiden und deren jeweiligen Energie- und Ressourcenbedarf erklären. Zudem können sie die Begriffe Operational Carbon und Embodied Carbon auf KI-Systeme anwenden.

[[LZ-8-2]]
==== LZ 8-2: Energieeffiziente KI-Architekturen bewerten

Die Teilnehmenden kennen grundlegende Architekturansätze für energieeffiziente KI-Systeme, z. B.:

* Edge- vs. Cloud-Inferenz
* Zentrale vs. verteilte Modellarchitekturen
* Batch- vs. Echtzeitverarbeitung
* Wiederverwendung vor Neutraining

Sie können diese Ansätze hinsichtlich Energieeffizienz, Kosten, Latenz und Genauigkeit vergleichen.

[[LZ-8-3]]
==== LZ 8-3: Optimierungstechniken für KI-Modelle kennen und einordnen

Die Teilnehmenden kennen gängige Techniken zur Reduktion des Ressourcenbedarfs von KI-Modellen, insbesondere:

* Transfer Learning und Fine-Tuning
* Quantization
* Pruning
* Knowledge Distillation
* Einsatz kleiner, spezialisierter Modelle statt großer Foundation Models

Sie können diese Techniken hinsichtlich Trainingsaufwand, Inferenzkosten, Modellgüte und Energieverbrauch bewerten.

[[LZ-8-4]]
==== LZ 8-4: Zielkonflikte zwischen Qualität und Energieeffizienz bei KI-Systemen erkennen

Die Teilnehmenden verstehen die Zielkonflikte zwischen klassischen Qualitätsattributen (z. B. Genauigkeit, Robustheit, Latenz, Skalierbarkeit) und Energieeffizienz bei KI-Systemen. Sie können diese Zielkonflikte explizit machen und in Architekturentscheidungen berücksichtigen. Die Teilnehmenden kennen die Konzepte Green AI und Sustainable AI. Sie verstehen, dass Effizienzgewinne zu einem Rebound-Effekt führen können und können Gegenmaßnahmen benennen.

// end::DE[]

// tag::EN[]

[[LG-8-1]]
==== LG 8-1: Understanding the environmental impact of AI systems

Participants understand the environmental impact of AI systems, particularly generative artificial intelligence with large language models, throughout their entire life cycle. They can distinguish between the phases of data collection, training, deployment, inference and maintenance, and explain their respective energy and resource requirements. They can also apply the terms operational carbon and embodied carbon to AI systems.

[[LG-8-2]]
==== LG 8-2: Evaluating energy-efficient AI architectures

Participants are familiar with basic architectural approaches for energy-efficient AI systems, e.g.:

* Edge vs. cloud inference
* Centralised vs. distributed model architectures
* Batch vs. real-time processing
* Reuse vs. retraining

They can compare these approaches in terms of energy efficiency, cost, latency and accuracy.

[[LG-8-3]]
==== LG 8-3: Knowing and classifying optimisation techniques for AI models

Participants are familiar with common techniques for reducing the resource requirements of AI models, in particular:

* Transfer learning and fine-tuning
* Quantisation
* Pruning
* Knowledge distillation
* Use of small, specialised models instead of large foundation models

They can evaluate these techniques in terms of training effort, inference costs, model quality and energy consumption.

[[LG-8-4]]
==== LG 8-4: Recognising trade-offs between quality and energy efficiency in AI systems

Participants understand the trade-offs between classic quality attributes (e.g. accuracy, robustness, latency, scalability) and energy efficiency in AI systems. They can make these trade-offs explicit and take them into account in architectural decisions. Participants are familiar with the concepts of green AI and sustainable AI. They understand that efficiency gains can lead to a rebound effect and can identify countermeasures.

// end::EN[]

